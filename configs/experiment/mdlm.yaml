# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default
  - override /data: pdb
  - override /model: default
  - override /trainer: deepspeed
  

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
callbacks:
  model_checkpoint:
    save_top_k: 1
    every_n_epochs: 1
    save_last: false
  early_stopping:
    patience: 100

data:
  batch_size: 16 # for 32GB GPU x 4

model:
  _target_: slm.models.model.MaskedDiffusionLanguageModeling
  compile: false
  optimizer:
    lr: 1e-5
  scheduler: null
    # _target_: transformers.get_constant_schedule_with_warmup
    # _partial_: true
    # num_warmup_steps: 2500
  noise_schedule: 
    _target_: slm.utils.noise_utils.LogLinearNoise
  T: 0
  noise_removal: true
  sampling_eps: 1e-3
  time_conditioning: true
  change_of_variables: false
  importance_sampling: false
  
  sequence_prediction: false 
  condition_dropout: 0.0
  condition_mask_rate: 0.0
  coupled_condition_mask: false

  structure_only: false

  sigma_embedder:
    _target_: slm.models.net.TimestepEmbedder
    hidden_size: 1536
  net: 
    _target_: slm.models.net.CustomizedESM3
    pretrained: true
    n_structure_heads: 4101
    n_sequence_heads: 0 # 32 is enough
  

trainer:
  min_epochs: 100
  max_epochs: 100
  # gradient_clip_val: 0.5


tags: ["pdb,esm3,finetune,dev"]

task_name: "MaskedDiffusionLanguageModeling"

seed: 42

ckpt_path: null