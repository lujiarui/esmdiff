# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default
  - override /data: pdb
  - override /model: default
  - override /trainer: ddp
  

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
callbacks:
  model_checkpoint:
    save_top_k: 1
    every_n_epochs: 5
    save_last: false
  early_stopping:
    patience: 100


data:
  batch_size: 16  # for 32GB GPU x 4

model:
  _target_: slm.models.model.ConditionalLanguageModeling
  optimizer:
    lr: 1e-5 
  net:
    config:
      initialize_emb_from_vq: true
      freeze_dec_emb: true
      dec_add_input_emb: true
      num_layers: 12
      vocab_size: 4101
      d_model: 1280
      d_ff: 2048


trainer:
  min_epochs: 100
  max_epochs: 100
  # gradient_clip_val: 0.5
  strategy: ddp


tags: ["pdb,transformer,dev"]

task_name: "ConditionalLanguageModeling"

seed: 42

ckpt_path: null